{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping without an API\n",
    "\n",
    "## Scraping Tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the packages we will need to begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to scrape the table from this website of box office data, so to begin, we name the url to a variable called url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.boxofficemojo.com/weekly/chart/?yr=2018&wk=01&p=.htm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of commands tells Python to open the web page and store the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get(url) ##opening the page and storing it to a variable called webpage\n",
    "page_content=webpage.content ##storing the content of the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can store the raw data of the page onto your computer, with the below commands. If you are downloading a lot of content, this will probably take up a lot of space on your computer, but it can be useful if you think you will want to go back and retrieve other content later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/Users/yotalao/Box Sync/2018-2019/CompSoc_Bootcamp/boxoffice.html', 'wb') ##opening a file to store the data , use 'wb' option for files that contain more than text\n",
    "f.write(page_content) ##writing data to the saved html page\n",
    "f.close() ##closing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the raw webpage, however, is not necessary; we can retrieve and clean the data all in the steps below. First, we want to \"beautify\" the html code on the page, so it is easily searchable with our package BeautifulSoup. This reveals the inherent structure of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_content, \"lxml\") ##beautifying it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the html looks like before beautifying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_content) ##ugly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the html looks like after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup) ##pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using Chrome's Inspect tool by right clicking on the table we want, we can figure out how to tell Python to retrieve that content. With the inspect tool, we can see that our table is marked with a table tag. However, there are multiple table tags, so we need to refer to the right table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=soup.findAll('table')[4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've downsized the content to the table we need, but we still need to clean it up. First, we'll extract all the row data, ignoring the column header (we'll deal with that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.findAll('tr')[1:93] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is still messy. We now need to extract the data in each cell. We do this using for loop statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data =[] \n",
    "for row in rows: \n",
    "    row_clean=[] \n",
    "    row_cells=row.findAll('td') \n",
    "    for row_cell in row_cells: \n",
    "        row_cell_clean=row_cell.getText() \n",
    "        row_clean.append(row_cell_clean) \n",
    "    movie_data.append(row_clean) \n",
    "print(movie_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a shorthand way to do the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data=[[cells.getText() for cells in rows[row].findAll('td')]\n",
    "            for row in range(len(rows))] ##getting text for each cell in cells extracted from each of the 92 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we work on extracting the column names. We see that this is in the first row tag of the table, so we tell Python to retrieve that row with the column header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=table.findAll('tr')[0] \n",
    "col_cells=cols.findAll('td')\n",
    "cols_clean=[] \n",
    "for col_cell in col_cells: \n",
    "    col_cell_clean=col_cell.getText()\n",
    "    cols_clean.append(col_cell_clean)\n",
    "print(cols_clean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a shorthand way to do the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [cell.getText() for cell in cols.findAll('td')] ##getting text for each cell in cells extracted from the row\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to put it into a data frame to analyze later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(movie_data, columns=cols) ##putting this into a pandas data frame, specifying what the column headers are\n",
    "##but uh oh, there's an error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work though. How do we fix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping other page content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the same packages as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to  to scrape news story headings from this website's science section, repeating many of the same steps as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.treehugger.com/science/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get(url) \n",
    "page_content=webpage.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_content, \"lxml\") ##beautifying it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the article content in the \"latest stories\" section. Using Inspect Element in Chrome, we see this content is contained under \"section class=\"c-block c-block--cards\"\" tag. Each story is in the \"article class=\"c-article c-article--card\"\" tag so we find this using beautiful soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest=soup.findAll(\"article\", class_=\"c-article c-article--card\") \n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to clean this up into a dictionary that can easily be converted into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_clean=[] ##this is going to become a list of dictionaries that can be easily converted into a pandas df\n",
    "for i in latest:\n",
    "    story={} ##creating a dictionary\n",
    "    story['headline']=i.find(class_=\"c-article__headline\").getText(strip=True) \n",
    "    story['category']=i.find(class_=\"c-article__category\").getText(strip=True)\n",
    "    story['author']=i.find(rel=\"author\").getText(strip=True)\n",
    "    story['pub_date']=i.find(class_=\"c-article__published\").getText(strip=True)\n",
    "    latest_clean.append(story)\n",
    "print(latest_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert it then into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(latest_clean)  ##converting to pandas dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Overview of Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all else fails and you're having difficulty using Beautiful Soup the content you need, regular expressions can be useful in finding data within text. Regular expressions are a language in their own right, so we don't have time to go over everything, but generally, they are expressions that find patterns in your text. \n",
    "\n",
    "In the above example, we could find the variables in the html code by using regular expressions. This method is generally less structured than finding the data with Beautiful Soup, so there is more room for error, but can be a useful strategy when used carefully.\n",
    "\n",
    "So, for example, in searching for the titles of each article in the Treehugger html code, we see that they are contained in a tag like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a class=\"gtm-track-click\" data-click-action=\"Promo Title Click\" data-click-category=\"Streams (Index Page)\" href=\"/plastic/if-bpa-so-terrible-why-everybody-still-drinking-beer-and-pop-out-bpa-lined-cans.html\">\n",
    "If BPA is so terrible, why is everybody still drinking beer and pop out of BPA lined cans?</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a regular expression that directs Python to retrieve anything that is in between the text \"html\">\" and \"</a\\>.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the regular expression package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the re.findall function to find the titles. We will first test this the small subset of data above, to make sure the regular expression is working properly. So first, we treat the small subset as a string and name it to the variable \"test.\" You can also test regular expressions in text editing programs like Text Wrangler, by using the find function and clicking \"grep.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"\"\"<a class=\"gtm-track-click\" data-click-action=\"Promo Title Click\" data-click-category=\"Streams (Index Page)\" href=\"/plastic/if-bpa-so-terrible-why-everybody-still-drinking-beer-and-pop-out-bpa-lined-cans.html\"> If BPA is so terrible, why is everybody still drinking beer and pop out of BPA lined cans?</a>\"\"\"\n",
    "test\n",
    "##we use three quotes at the beginning and end of the the string so Python knows it's a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test out regular expressions in searching for text. \".*?\" is a regular expression that is called a lazy quantifier. It matches the text in between what is before and after it the first time it encounters it and then stops (as opposed to other regular expressions that are \"greedy\" and match the expression as many times they appear). We can use parentheses to tell Python only to return the text that is in between the parentheses. \"\\s\" indicates a single space; we do not Python to return the space, so we include it outside of the parentheses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = re.findall('html\">\\s(.*?)</a>', test) ##this tells it to return everything in between\n",
    "##html\"> and </a>\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked, so now we will use it on our larger portion of html code called \"latest\" to get all the titles on the html page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_string=str(soup)\n",
    "titles=re.findall('html\">\\s(.*)<\\/a>', soup_string)\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try scraping  the UCLA Faculty Webpage: https://soc.ucla.edu/faculty. Scrape each faculty members name and at least three other variables (such as title, email, subfields, etc.) and clean the data into a Pandas data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
